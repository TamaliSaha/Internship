{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccae2673",
   "metadata": {},
   "source": [
    "# Webscraping Assignment 1\n",
    "\n",
    "### Submitted by Tamali Saha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afea0c",
   "metadata": {},
   "source": [
    "## 1) Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8896b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a50591bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 -> Main Page\n",
      "h1 -> Welcome to Wikipedia\n",
      "h2 -> From today's featured article\n",
      "h2 -> Did you know ...\n",
      "h2 -> In the news\n",
      "h2 -> On this day\n",
      "h2 -> From today's featured list\n",
      "h2 -> Today's featured picture\n",
      "h2 -> Other areas of Wikipedia\n",
      "h2 -> Wikipedia's sister projects\n",
      "h2 -> Wikipedia languages\n",
      "h2 -> Navigation menu\n",
      "h3 -> Personal tools\n",
      "h3 -> Namespaces\n",
      "h3 -> Views\n",
      "h3 -> Search\n",
      "h3 -> Navigation\n",
      "h3 -> Contribute\n",
      "h3 -> Tools\n",
      "h3 -> Print/export\n",
      "h3 -> In other projects\n",
      "h3 -> Languages\n"
     ]
    }
   ],
   "source": [
    "request = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "Soup = BeautifulSoup(request.text, 'lxml')\n",
    "heading_tags = [\"h1\", \"h2\", \"h3\",'h4','h5','h6']\n",
    "for tags in Soup.find_all(heading_tags):\n",
    "    print(tags.name + ' -> ' + tags.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab78f3",
   "metadata": {},
   "source": [
    "## 2)Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99c695c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB’s Top rated 100 movies’ data \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Jagten</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Idi i smotri</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.2</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Movie Name Ratings  Year\n",
       "1             The Shawshank Redemption     9.2  1994\n",
       "2                        The Godfather     9.2  1972\n",
       "3                      The Dark Knight     9.0  2008\n",
       "4                The Godfather Part II     9.0  1974\n",
       "5                         12 Angry Men     8.9  1957\n",
       "..                                 ...     ...   ...\n",
       "96                              Jagten     8.3  2012\n",
       "97   M - Eine Stadt sucht einen Mörder     8.3  1931\n",
       "98                  North by Northwest     8.3  1959\n",
       "99                        Idi i smotri     8.2  1985\n",
       "100                            Vertigo     8.2  1958\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.imdb.com/chart/top/\"\n",
    "page = requests.get(url)\n",
    "# display the page content\n",
    "soup= BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "#scrap and parse movie names\n",
    "\n",
    "scrapped_movies= soup.find_all(\"td\",class_=\"titleColumn\")\n",
    "movies=[]\n",
    "for i in scrapped_movies:\n",
    "    i= i.a.text.replace('\\n',\"\").strip(\" \")\n",
    "    movies.append(i)\n",
    "    \n",
    "#scrap and parse year\n",
    "\n",
    "scrapped_years= soup.find_all(\"td\",class_=\"titleColumn\")\n",
    "years=[]\n",
    "for i in scrapped_years:\n",
    "    i= i.span.text.strip('()')\n",
    "    years.append(i)\n",
    "    \n",
    "#scrap and parse ratings\n",
    "\n",
    "scrapped_ratings= soup.find_all(\"td\",class_=\"ratingColumn imdbRating\")\n",
    "ratings=[]\n",
    "for i in scrapped_ratings:\n",
    "    ratings.append(i.get_text().replace('\\n',\"\"))\n",
    "    \n",
    "# Store the scrapped data\n",
    "data= pd.DataFrame()\n",
    "data[\"Movie Name\"]= movies\n",
    "data[\"Ratings\"]= ratings\n",
    "data[\"Year\"] = years\n",
    "import numpy as np\n",
    "x= np.arange(1,101,1)\n",
    "\n",
    "# Final result of top 100 movies name, year and rating\n",
    "top100_movies = data.head(100)\n",
    "\n",
    "top100_movies= top100_movies.set_index(x)\n",
    "\n",
    "top100_movies.to_csv(\"IDBM Top 100 movies.csv\", index= False)\n",
    "print( \"IMDB’s Top rated 100 movies’ data \\n\")\n",
    "top100_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e352817",
   "metadata": {},
   "source": [
    "## 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4453114f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB’s Top rated 100 Indian movies’ data \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Golmaal</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.4</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jai Bhim</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Angoor</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Baasha</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Virumandi</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Movie Name Ratings  Year\n",
       "1     Rocketry: The Nambi Effect     8.5  2022\n",
       "2                     Anbe Sivam     8.4  2003\n",
       "3                        Golmaal     8.4  1979\n",
       "4                        Nayakan     8.4  1987\n",
       "5                       Jai Bhim     8.4  2021\n",
       "..                           ...     ...   ...\n",
       "96                        Angoor     8.0  1982\n",
       "97               Rang De Basanti     8.0  2006\n",
       "98                        Baasha     8.0  1995\n",
       "99   Baahubali 2: The Conclusion     8.0  2017\n",
       "100                    Virumandi     8.0  2004\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.imdb.com/india/top-rated-indian-movies/\"\n",
    "page1 = requests.get(url)\n",
    "soup= BeautifulSoup(page1.content, \"html.parser\")\n",
    "\n",
    "#scrap and parse movie names\n",
    "\n",
    "scrapped_movies= soup.find_all(\"td\",class_=\"titleColumn\")\n",
    "movies=[]\n",
    "for i in scrapped_movies:\n",
    "    i= i.a.text.replace('\\n',\"\").strip(\" \")\n",
    "    movies.append(i)\n",
    "    \n",
    "#scrap and parse ratings\n",
    "\n",
    "scrapped_ratings= soup.find_all(\"td\",class_=\"ratingColumn imdbRating\")\n",
    "ratings=[]\n",
    "for i in scrapped_ratings:\n",
    "    ratings.append(i.get_text().replace('\\n',\"\"))\n",
    "    \n",
    "#scrap and parse year\n",
    "\n",
    "scrapped_years= soup.find_all(\"td\",class_=\"titleColumn\")\n",
    "years=[]\n",
    "for i in scrapped_years:\n",
    "    i= i.span.text.strip('()')\n",
    "    years.append(i)\n",
    "    \n",
    "# Store the scrapped data of top 100 indian movies\n",
    "data= pd.DataFrame()\n",
    "data[\"Movie Name\"]= movies\n",
    "data[\"Ratings\"]= ratings\n",
    "data[\"Year\"] = years\n",
    "import numpy as np\n",
    "x= np.arange(1,101,1)\n",
    "\n",
    "# Final result of top 100 movies name, year and rating\n",
    "\n",
    "top100_indian_movies = data.head(100)\n",
    "\n",
    "top100_indian_movies= top100_indian_movies.set_index(x)\n",
    "\n",
    "top100_indian_movies.to_csv(\"IDBM Top 100 Indian movies.csv\", index= False)\n",
    "print( \"IMDB’s Top rated 100 Indian movies’ data \\n\")\n",
    "top100_indian_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab73ea",
   "metadata": {},
   "source": [
    "## 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b7eaff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of respected former presidents of India\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>President Name</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   President Name  \\\n",
       "0           Shri Ram Nath Kovind    \n",
       "1          Shri Pranab Mukherjee    \n",
       "2   Smt Pratibha Devisingh Patil    \n",
       "3         DR. A.P.J. Abdul Kalam    \n",
       "4           Shri K. R. Narayanan    \n",
       "5        Dr Shankar Dayal Sharma    \n",
       "6            Shri R Venkataraman    \n",
       "7               Giani Zail Singh    \n",
       "8      Shri Neelam Sanjiva Reddy    \n",
       "9       Dr. Fakhruddin Ali Ahmed    \n",
       "10  Shri Varahagiri Venkata Giri    \n",
       "11              Dr. Zakir Husain    \n",
       "12  Dr. Sarvepalli Radhakrishnan    \n",
       "13          Dr. Rajendra Prasad     \n",
       "\n",
       "                                       Term of Office  \n",
       "0                     25 July, 2017 to 25 July, 2022   \n",
       "1                     25 July, 2012 to 25 July, 2017   \n",
       "2                     25 July, 2007 to 25 July, 2012   \n",
       "3                     25 July, 2002 to 25 July, 2007   \n",
       "4                     25 July, 1997 to 25 July, 2002   \n",
       "5                     25 July, 1992 to 25 July, 1997   \n",
       "6                     25 July, 1987 to 25 July, 1992   \n",
       "7                     25 July, 1982 to 25 July, 1987   \n",
       "8                     25 July, 1977 to 25 July, 1982   \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10   3 May, 1969 to 20 July, 1969 and 24 August, 1...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "page1 = requests.get(url)\n",
    "soup= BeautifulSoup(page1.content, \"html.parser\")\n",
    "\n",
    "\n",
    "#scrap and parse president names\n",
    "\n",
    "scrapped_names= soup.find_all(\"div\",class_=\"presidentListing\")\n",
    "name=[]\n",
    "president_names=[]\n",
    "for i in scrapped_names:\n",
    "    i= i.h3.text\n",
    "    name.append(i)\n",
    "for i in name:\n",
    "    string=re.sub(\"\\(.*?\\)\",\"()\",i).replace(\"()\",\"\")\n",
    "    president_names.append(string)\n",
    "\n",
    "#scrap and parse Term of office\n",
    "\n",
    "scrapped_terms= soup.find_all(\"p\")[:18]\n",
    "terms=[]\n",
    "Term_of_office=[]\n",
    "for i in scrapped_terms:\n",
    "    i= i.text.replace('\\n',\"\"). replace(\"Term of Office:\", \"\")\n",
    "    terms.append(i)\n",
    "for i in terms:\n",
    "    result = re.sub(r'http\\S+',\"\", i)\n",
    "    Term_of_office.append(result)\n",
    "    Term_of_office= list(filter(None,Term_of_office))\n",
    "    \n",
    "#display\n",
    "Presidentlist= pd.DataFrame()\n",
    "Presidentlist[\"President Name\"]= president_names\n",
    "Presidentlist[\"Term of Office\"]= Term_of_office\n",
    "Presidentlist.to_csv(\"Formar Indian Presedent.csv\", index= False)\n",
    "print (\"List of respected former presidents of India\")\n",
    "Presidentlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3c9eac",
   "metadata": {},
   "source": [
    "## 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43b50d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "page3 = requests.get(url)\n",
    "soup= BeautifulSoup(page3.content, \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23b3bdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New Zealand',\n",
       " 'England',\n",
       " 'India',\n",
       " 'Pakistan',\n",
       " 'Australia',\n",
       " 'South Africa',\n",
       " 'Bangladesh',\n",
       " 'Sri Lanka',\n",
       " 'West Indies',\n",
       " 'Afghanistan']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scrap and parse team name\n",
    "team= soup.find_all(\"span\",class_=\"u-hide-phablet\")[:10]\n",
    "teams=[]\n",
    "for i in team:\n",
    "    i= i.text\n",
    "    teams.append(i)\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "887ee9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19', '27', '31', '22', '25', '21', '30', '29', '41', '18']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##scrap and parse matches\n",
    "\n",
    "matchhead = soup.find(\"td\",class_=\"rankings-block__banner--matches\")\n",
    "ss= matchhead.text\n",
    "jj= list(ss.split(\" \"))\n",
    "\n",
    "m= soup.find_all(\"td\",class_=\"table-body__cell u-center-text\")\n",
    "ms=[]\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    ms.append(i)\n",
    "    \n",
    "for j in range(0, len(ms)): \n",
    "    if j % 2== 0:\n",
    "        jj.append(ms[j])\n",
    "        \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_men_matches = jj[0:10]\n",
    "top10_men_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1504d826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2,355',\n",
       " '3,226',\n",
       " '3,447',\n",
       " '2,354',\n",
       " '2,548',\n",
       " '2,111',\n",
       " '2,753',\n",
       " '2,658',\n",
       " '2,902',\n",
       " '1,238']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##scrap and parse points\n",
    "\n",
    "pointhead = soup.find(\"td\",class_=\"rankings-block__banner--points\")\n",
    "ss= pointhead.text\n",
    "jj= list(ss.split(\" \"))\n",
    "\n",
    "point= soup.find_all(\"td\",class_=\"table-body__cell u-center-text\")\n",
    "points=[]\n",
    "\n",
    "for i in point:\n",
    "    i= i.text\n",
    "    points.append(i)\n",
    "    \n",
    "for j in range(0, len(points)):\n",
    "    if j % 2== 1:\n",
    "        jj.append(points[j])\n",
    "        \n",
    "#we need to fetch top 10\n",
    "top10_men_point = jj[0:10]\n",
    "top10_men_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abba799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['124', '119', '111', '107', '102', '101', '92', '92', '71', '69']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##scrap and parse rating\n",
    "\n",
    "ratinghead = soup.find(\"td\",class_=\"rankings-block__banner--rating u-text-right\")\n",
    "ss= ratinghead.text.replace('\\n',\"\").strip()\n",
    "jj= list(ss.split(\" \"))\n",
    "\n",
    "point= soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\")\n",
    "for i in point:\n",
    "    i= i.text\n",
    "    jj.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_men_rating = jj[0:10]\n",
    "top10_men_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb37d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket along with the records for matches, points and rating :\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team Name</th>\n",
       "      <th>Top_Men_Matches</th>\n",
       "      <th>Top_Men_Points</th>\n",
       "      <th>Top_Men_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>19</td>\n",
       "      <td>2,355</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>27</td>\n",
       "      <td>3,226</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India</td>\n",
       "      <td>31</td>\n",
       "      <td>3,447</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>25</td>\n",
       "      <td>2,548</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>21</td>\n",
       "      <td>2,111</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>29</td>\n",
       "      <td>2,658</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>18</td>\n",
       "      <td>1,238</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Team Name Top_Men_Matches Top_Men_Points Top_Men_Rating\n",
       "0   New Zealand              19          2,355            124\n",
       "1       England              27          3,226            119\n",
       "2         India              31          3,447            111\n",
       "3      Pakistan              22          2,354            107\n",
       "4     Australia              25          2,548            102\n",
       "5  South Africa              21          2,111            101\n",
       "6    Bangladesh              30          2,753             92\n",
       "7     Sri Lanka              29          2,658             92\n",
       "8   West Indies              41          2,902             71\n",
       "9   Afghanistan              18          1,238             69"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.DataFrame()\n",
    "data[\"Team Name\"]= teams\n",
    "data[\"Top_Men_Matches\"]= top10_men_matches\n",
    "data[\"Top_Men_Points\"]= top10_men_point\n",
    "data[\"Top_Men_Rating\"]= top10_men_rating\n",
    "\n",
    "print (\"Top 10 ODI teams in men’s cricket along with the records for matches, points and rating :\\n \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d89e76",
   "metadata": {},
   "source": [
    "## 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d8b3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "page4 = requests.get(url)\n",
    "soup= BeautifulSoup(page4.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "330c1dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Babar Azam',\n",
       " 'Rassie van der Dussen',\n",
       " 'Quinton de Kock',\n",
       " 'Imam-ul-Haq',\n",
       " 'Virat Kohli',\n",
       " 'Rohit Sharma',\n",
       " 'David Warner',\n",
       " 'Jonny Bairstow',\n",
       " 'Ross Taylor',\n",
       " 'Aaron Finch']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##scrap and parse batsman\n",
    "playerhead = soup.find(\"div\",class_=\"rankings-block__banner--name-large\")\n",
    "ss= playerhead.text\n",
    "top_batsman= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rankings-table__name name\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text. replace(\"\\n\",'')\n",
    "    top_batsman.append(i)\n",
    "    \n",
    "#we need to fetch top 10    \n",
    "top10_batsman =top_batsman[0:10]\n",
    "top10_batsman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d4de47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAK', 'SA', 'SA', 'PAK', 'IND', 'IND', 'AUS', 'ENG', 'NZ', 'AUS']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##scrap and parse teamname\n",
    "teamhead = soup.find(\"div\",class_=\"rankings-block__banner--nationality\")\n",
    "ss= teamhead.text.replace(\"\\n\", '')\n",
    "teams= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    teams.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_team =teams[0:10]\n",
    "top10_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "335229c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['890', '789', '784', '779', '744', '740', '739', '732', '722', '706']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##scrap and parse ratings\n",
    "ratinghead = soup.find(\"div\",class_=\"rankings-block__banner--rating\")\n",
    "ss= ratinghead.text.replace(\"\\n\", '')\n",
    "ratings= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rating\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    ratings.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_rating =ratings[0:10]\n",
    "top10_rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fd52f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Batsmen along with the records of their team and rating :\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_Men_Batsman</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Top_Men_Batsman Team Rating\n",
       "0             Babar Azam  PAK    890\n",
       "1  Rassie van der Dussen   SA    789\n",
       "2        Quinton de Kock   SA    784\n",
       "3            Imam-ul-Haq  PAK    779\n",
       "4            Virat Kohli  IND    744\n",
       "5           Rohit Sharma  IND    740\n",
       "6           David Warner  AUS    739\n",
       "7         Jonny Bairstow  ENG    732\n",
       "8            Ross Taylor   NZ    722\n",
       "9            Aaron Finch  AUS    706"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.DataFrame()\n",
    "\n",
    "data[\"Top_Men_Batsman\"]= top10_batsman\n",
    "data[\"Team\"]= top10_team\n",
    "data[\"Rating\"]= top10_rating\n",
    "\n",
    "print (\"Top 10 ODI Batsmen along with the records of their team and rating :\\n \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed57141",
   "metadata": {},
   "source": [
    "## 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1129dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI bowlers along with the records of their team and rating :\n",
      "\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_Men_Bowlers</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mustafizur Rahman</td>\n",
       "      <td>BAN</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Top_Men_Bowlers Team Rating\n",
       "0        Trent Boult   NZ    720\n",
       "1     Josh Hazlewood  AUS    678\n",
       "2   Mujeeb Ur Rahman  AFG    676\n",
       "3     Jasprit Bumrah  IND    662\n",
       "4     Shaheen Afridi  PAK    661\n",
       "5      Mohammad Nabi  AFG    657\n",
       "6       Mehedi Hasan  BAN    655\n",
       "7        Rashid Khan  AFG    651\n",
       "8         Matt Henry   NZ    644\n",
       "9  Mustafizur Rahman  BAN    640"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "page5 = requests.get(url)\n",
    "soup= BeautifulSoup(page5.content, \"html.parser\")\n",
    "\n",
    "##scrap and parse  bowlers\n",
    "bowlerhead = soup.find(\"div\",class_=\"rankings-block__banner--name-large\")\n",
    "ss= bowlerhead.text\n",
    "top_bowler= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rankings-table__name name\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text. replace(\"\\n\",'')\n",
    "    top_bowler.append(i)\n",
    "\n",
    "#we need to fetch top 10    \n",
    "top10_bowler =top_bowler[0:10]\n",
    "\n",
    "##scrap and parse teamname\n",
    "teamhead = soup.find(\"div\",class_=\"rankings-block__banner--nationality\")\n",
    "ss= teamhead.text.replace(\"\\n\", '')\n",
    "teams= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    teams.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_team =teams[0:10]\n",
    "\n",
    "##scrap and parse ratings\n",
    "ratinghead = soup.find(\"div\",class_=\"rankings-block__banner--rating\")\n",
    "ss= ratinghead.text.replace(\"\\n\", '')\n",
    "ratings= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rating\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    ratings.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_rating =ratings[0:10]\n",
    "\n",
    "#display\n",
    "data= pd.DataFrame()\n",
    "\n",
    "data[\"Top_Men_Bowlers\"]= top10_bowler\n",
    "data[\"Team\"]= top10_team\n",
    "data[\"Rating\"]= top10_rating\n",
    "\n",
    "print (\"Top 10 ODI bowlers along with the records of their team and rating :\\n\\n \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14e3a8",
   "metadata": {},
   "source": [
    "## 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b037a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women’s cricket along with the records for matches, points and rating :\n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Women's Team Name</th>\n",
       "      <th>Matches</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>4,837</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>33</td>\n",
       "      <td>4,046</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>35</td>\n",
       "      <td>4,157</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>32</td>\n",
       "      <td>3,219</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>31</td>\n",
       "      <td>3,019</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>30</td>\n",
       "      <td>2,768</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>930</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>30</td>\n",
       "      <td>1,962</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>11</td>\n",
       "      <td>516</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>11</td>\n",
       "      <td>495</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Women's Team Name Matches Points Rating\n",
       "0         Australia      29  4,837    167\n",
       "1           England      33  4,046    123\n",
       "2      South Africa      35  4,157    119\n",
       "3             India      32  3,219    101\n",
       "4       New Zealand      31  3,019     97\n",
       "5       West Indies      30  2,768     92\n",
       "6        Bangladesh      12    930     78\n",
       "7          Pakistan      30  1,962     65\n",
       "8           Ireland      11    516     47\n",
       "9         Sri Lanka      11    495     45"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "page6 = requests.get(url)\n",
    "soup= BeautifulSoup(page6.content, \"html.parser\")\n",
    "\n",
    "#scrap and parse women's team name\n",
    "team= soup.find_all(\"span\",class_=\"u-hide-phablet\")[:10]\n",
    "women_teams=[]\n",
    "for i in team:\n",
    "    i= i.text\n",
    "    women_teams.append(i)\n",
    "\n",
    "##scrap and parse matches\n",
    "\n",
    "matchhead = soup.find(\"td\",class_=\"rankings-block__banner--matches\")\n",
    "ss= matchhead.text\n",
    "jj= list(ss.split(\" \"))\n",
    "\n",
    "m= soup.find_all(\"td\",class_=\"table-body__cell u-center-text\")\n",
    "ms=[]\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    ms.append(i)\n",
    "    \n",
    "for j in range(0, len(ms)): \n",
    "    if j % 2== 0:\n",
    "        jj.append(ms[j])\n",
    "        \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_women_matches = jj[0:10]\n",
    "\n",
    "##scrap and parse points\n",
    "\n",
    "pointhead = soup.find(\"td\",class_=\"rankings-block__banner--points\")\n",
    "ss= pointhead.text\n",
    "jj= list(ss.split(\" \"))\n",
    "\n",
    "point= soup.find_all(\"td\",class_=\"table-body__cell u-center-text\")\n",
    "points=[]\n",
    "\n",
    "for i in point:\n",
    "    i= i.text\n",
    "    points.append(i)\n",
    "    \n",
    "for j in range(0, len(points)):\n",
    "    if j % 2== 1:\n",
    "        jj.append(points[j])\n",
    "        \n",
    "#we need to fetch top 10\n",
    "top10_women_point = jj[0:10]\n",
    "\n",
    "##scrap and parse rating\n",
    "\n",
    "ratinghead = soup.find(\"td\",class_=\"rankings-block__banner--rating u-text-right\")\n",
    "ss= ratinghead.text.replace('\\n',\"\").strip()\n",
    "jj= list(ss.split(\" \"))\n",
    "\n",
    "point= soup.find_all(\"td\",class_=\"table-body__cell u-text-right rating\")\n",
    "for i in point:\n",
    "    i= i.text\n",
    "    jj.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_women_rating = jj[0:10]\n",
    "\n",
    "#display\n",
    "data= pd.DataFrame()\n",
    "data[\"Women's Team Name\"]= women_teams\n",
    "data[\"Matches\"]= top10_women_matches\n",
    "data[\"Points\"]= top10_women_point\n",
    "data[\"Rating\"]= top10_women_rating\n",
    "\n",
    "print (\"Top 10 ODI teams in women’s cricket along with the records for matches, points and rating :\\n \\n\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aae89d",
   "metadata": {},
   "source": [
    "## 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1846974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI Batting players along with the records of their team and rating :\n",
      " \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_Women_Batsman</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Top_Women_Batsman Team Rating\n",
       "0         Alyssa Healy  AUS    785\n",
       "1          Beth Mooney  AUS    749\n",
       "2       Natalie Sciver  ENG    747\n",
       "3      Laura Wolvaardt   SA    732\n",
       "4          Meg Lanning  AUS    710\n",
       "5       Rachael Haynes  AUS    701\n",
       "6    Amy Satterthwaite   NZ    681\n",
       "7       Tammy Beaumont  ENG    667\n",
       "8  Chamari Athapaththu   SL    655\n",
       "9      Smriti Mandhana  IND    649"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "page6 = requests.get(url)\n",
    "soup= BeautifulSoup(page6.content, \"html.parser\")\n",
    "\n",
    "##scrap and parse women batsman\n",
    "playerhead = soup.find(\"div\",class_=\"rankings-block__banner--name-large\")\n",
    "ss= playerhead.text\n",
    "women_batsman= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rankings-table__name name\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text. replace(\"\\n\",'')\n",
    "    women_batsman.append(i)\n",
    "    \n",
    "#we need to fetch top 10    \n",
    "top10_women_batsman =women_batsman[0:10]\n",
    "\n",
    "\n",
    "##scrap and parse teamname\n",
    "teamhead = soup.find(\"div\",class_=\"rankings-block__banner--nationality\")\n",
    "ss= teamhead.text.replace(\"\\n\", '')\n",
    "teams= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    teams.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_women_team =teams[0:10]\n",
    "\n",
    "\n",
    "##scrap and parse ratings\n",
    "ratinghead = soup.find(\"div\",class_=\"rankings-block__banner--rating\")\n",
    "ss= ratinghead.text.replace(\"\\n\", '')\n",
    "ratings= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rating\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    ratings.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_women_rating =ratings[0:10]\n",
    "\n",
    "\n",
    "#display\n",
    "\n",
    "data= pd.DataFrame()\n",
    "data[\"Top_Women_Batsman\"]= top10_women_batsman\n",
    "data[\"Team\"]= top10_women_team\n",
    "data[\"Rating\"]= top10_women_rating\n",
    "\n",
    "print (\"Top 10 women’s ODI Batting players along with the records of their team and rating :\\n \\n \")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7dd1a1",
   "metadata": {},
   "source": [
    "## 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b260043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 women’s ODI all-rounder along with the records of their team and rating :\n",
      "\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top_Women_Allrounder</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Top_Women_Allrounder Team Rating\n",
       "0       Natalie Sciver  ENG    379\n",
       "1         Ellyse Perry  AUS    374\n",
       "2       Marizanne Kapp   SA    349\n",
       "3      Hayley Matthews   WI    339\n",
       "4          Amelia Kerr   NZ    336\n",
       "5     Ashleigh Gardner  AUS    270\n",
       "6        Deepti Sharma  IND    252\n",
       "7        Jess Jonassen  AUS    246\n",
       "8      Katherine Brunt  ENG    220\n",
       "9      Stafanie Taylor   WI    207"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "page5 = requests.get(url)\n",
    "soup= BeautifulSoup(page5.content, \"html.parser\")\n",
    "\n",
    "##scrap and parse all rounder\n",
    "allrounderhead = soup.find(\"div\",class_=\"rankings-block__banner--name-large\")\n",
    "ss= allrounderhead.text\n",
    "top_allrounder= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rankings-table__name name\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text. replace(\"\\n\",'')\n",
    "    top_allrounder.append(i)\n",
    "\n",
    "#we need to fetch top 10    \n",
    "top10_allrounder =top_allrounder[0:10]\n",
    "\n",
    "##scrap and parse teamname\n",
    "teamhead = soup.find(\"div\",class_=\"rankings-block__banner--nationality\")\n",
    "ss= teamhead.text.replace(\"\\n\", '')\n",
    "teams= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"span\", class_=\"table-body__logo-text\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    teams.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_allrounder_team =teams[0:10]\n",
    "\n",
    "##scrap and parse ratings\n",
    "ratinghead = soup.find(\"div\",class_=\"rankings-block__banner--rating\")\n",
    "ss= ratinghead.text.replace(\"\\n\", '')\n",
    "ratings= list(ss.split(\"\\n \"))\n",
    "\n",
    "m= soup.find_all(\"td\", class_=\"table-body__cell rating\")\n",
    "\n",
    "for i in m:\n",
    "    i= i.text\n",
    "    ratings.append(i)\n",
    "    \n",
    "#we need to fetch top 10\n",
    "\n",
    "top10_allrounder_rating =ratings[0:10]\n",
    "\n",
    "#display\n",
    "data= pd.DataFrame()\n",
    "\n",
    "data[\"Top_Women_Allrounder\"]= top10_allrounder\n",
    "data[\"Team\"]= top10_allrounder_team\n",
    "data[\"Rating\"]= top10_allrounder_rating\n",
    "\n",
    "print (\"Top 10 women’s ODI all-rounder along with the records of their team and rating :\\n\\n \")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df787f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "235eae88",
   "metadata": {},
   "source": [
    "## 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7faa2d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News details from https://www.cnbc.com : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low vol ETFs are beating the market this year....</td>\n",
       "      <td>31 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/low-vol-etfs-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fox News personalities face questioning as Dom...</td>\n",
       "      <td>55 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/fox-news-perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GM offers to buy out Buick dealers that don't ...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/gm-offers-to-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VR partnership with Meta is another way Qualco...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/virtual-realit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>August jobs report shows declining labor force...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/august-jobs-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stocks making the biggest moves midday: Kohl's...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BA.5 Covid boosters could come as soon as Labo...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/-omicron-covid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Huawei spin-off Honor to launch first foldable...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/honor-launches...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'Why shouldn't it be as bad as the 1970s?': Ni...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/1970s-inflatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NYC sues Starbucks for coffee chain's firing o...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/nyc-sues-starb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>White House fires back at Republican plan to c...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/white-house-sl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Student loan forgiveness could result in a $2,...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/student-loan-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Crypto prices are down, but it's not scaring a...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/crypto-winter-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>August jobs report shows a 'soft landing' by t...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/august-jobs-re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Cathie Wood doubles down on Nvidia, Innovation...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/cathie-woods-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Four dozen empty folders marked 'CLASSIFIED' f...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/trump-fbi-raid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How surviving cancer changed this 34-year-old'...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/how-cancer-cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Starbucks CEO transition plan gets muted react...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/starbucks-ceo-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A fitness trainer shares the 7 stretches she d...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/a-fitness-trai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Here's where the jobs are for August 2022 — in...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/heres-where-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Consider seeking shelter in dividend growers o...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/consider-seeki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31-year-old makes $15,000/mo recording voiceov...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/millennial-qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stiglitz says there are 3 reasons why Fed hike...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/joseph-stiglit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1 in 5 home sellers is now dropping their aski...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/more-home-sell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>These are the top 10 safest countries for solo...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/top-10-safest-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>No, McDonald's all-day breakfast isn't returni...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/no-mcdonalds-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ford's new vehicle sales slow in August, in li...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/fords-new-vehi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>G-7 nations back plan to cap Russian oil price...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/g-7-nations-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>A fashion company wants to buy your pumpkin sp...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/fashion-retail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Interest rates for high-yield savings accounts...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/09/02/high-yield-sav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline         Time  \\\n",
       "0   Low vol ETFs are beating the market this year....   31 Min Ago   \n",
       "1   Fox News personalities face questioning as Dom...   55 Min Ago   \n",
       "2   GM offers to buy out Buick dealers that don't ...   1 Hour Ago   \n",
       "3   VR partnership with Meta is another way Qualco...   1 Hour Ago   \n",
       "4   August jobs report shows declining labor force...   1 Hour Ago   \n",
       "5   Stocks making the biggest moves midday: Kohl's...   1 Hour Ago   \n",
       "6   BA.5 Covid boosters could come as soon as Labo...   1 Hour Ago   \n",
       "7   Huawei spin-off Honor to launch first foldable...   1 Hour Ago   \n",
       "8   'Why shouldn't it be as bad as the 1970s?': Ni...   1 Hour Ago   \n",
       "9   NYC sues Starbucks for coffee chain's firing o...  2 Hours Ago   \n",
       "10  White House fires back at Republican plan to c...  2 Hours Ago   \n",
       "11  Student loan forgiveness could result in a $2,...  2 Hours Ago   \n",
       "12  Crypto prices are down, but it's not scaring a...  2 Hours Ago   \n",
       "13  August jobs report shows a 'soft landing' by t...  2 Hours Ago   \n",
       "14  Cathie Wood doubles down on Nvidia, Innovation...  2 Hours Ago   \n",
       "15  Four dozen empty folders marked 'CLASSIFIED' f...  3 Hours Ago   \n",
       "16  How surviving cancer changed this 34-year-old'...  3 Hours Ago   \n",
       "17  Starbucks CEO transition plan gets muted react...  3 Hours Ago   \n",
       "18  A fitness trainer shares the 7 stretches she d...  3 Hours Ago   \n",
       "19  Here's where the jobs are for August 2022 — in...  3 Hours Ago   \n",
       "20  Consider seeking shelter in dividend growers o...  3 Hours Ago   \n",
       "21  31-year-old makes $15,000/mo recording voiceov...  3 Hours Ago   \n",
       "22  Stiglitz says there are 3 reasons why Fed hike...  3 Hours Ago   \n",
       "23  1 in 5 home sellers is now dropping their aski...  3 Hours Ago   \n",
       "24  These are the top 10 safest countries for solo...  3 Hours Ago   \n",
       "25  No, McDonald's all-day breakfast isn't returni...  4 Hours Ago   \n",
       "26  Ford's new vehicle sales slow in August, in li...  4 Hours Ago   \n",
       "27  G-7 nations back plan to cap Russian oil price...  4 Hours Ago   \n",
       "28  A fashion company wants to buy your pumpkin sp...  4 Hours Ago   \n",
       "29  Interest rates for high-yield savings accounts...  4 Hours Ago   \n",
       "\n",
       "                                            News Link  \n",
       "0   https://www.cnbc.com/2022/09/02/low-vol-etfs-a...  \n",
       "1   https://www.cnbc.com/2022/09/02/fox-news-perso...  \n",
       "2   https://www.cnbc.com/2022/09/02/gm-offers-to-b...  \n",
       "3   https://www.cnbc.com/2022/09/02/virtual-realit...  \n",
       "4   https://www.cnbc.com/2022/09/02/august-jobs-re...  \n",
       "5   https://www.cnbc.com/2022/09/02/stocks-making-...  \n",
       "6   https://www.cnbc.com/2022/09/02/-omicron-covid...  \n",
       "7   https://www.cnbc.com/2022/09/02/honor-launches...  \n",
       "8   https://www.cnbc.com/2022/09/02/1970s-inflatio...  \n",
       "9   https://www.cnbc.com/2022/09/02/nyc-sues-starb...  \n",
       "10  https://www.cnbc.com/2022/09/02/white-house-sl...  \n",
       "11  https://www.cnbc.com/2022/09/02/student-loan-f...  \n",
       "12  https://www.cnbc.com/2022/09/02/crypto-winter-...  \n",
       "13  https://www.cnbc.com/2022/09/02/august-jobs-re...  \n",
       "14  https://www.cnbc.com/2022/09/02/cathie-woods-a...  \n",
       "15  https://www.cnbc.com/2022/09/02/trump-fbi-raid...  \n",
       "16  https://www.cnbc.com/2022/09/02/how-cancer-cha...  \n",
       "17  https://www.cnbc.com/2022/09/02/starbucks-ceo-...  \n",
       "18  https://www.cnbc.com/2022/09/02/a-fitness-trai...  \n",
       "19  https://www.cnbc.com/2022/09/02/heres-where-th...  \n",
       "20  https://www.cnbc.com/2022/09/02/consider-seeki...  \n",
       "21  https://www.cnbc.com/2022/09/02/millennial-qui...  \n",
       "22  https://www.cnbc.com/2022/09/02/joseph-stiglit...  \n",
       "23  https://www.cnbc.com/2022/09/02/more-home-sell...  \n",
       "24  https://www.cnbc.com/2022/09/02/top-10-safest-...  \n",
       "25  https://www.cnbc.com/2022/09/02/no-mcdonalds-a...  \n",
       "26  https://www.cnbc.com/2022/09/02/fords-new-vehi...  \n",
       "27  https://www.cnbc.com/2022/09/02/g-7-nations-ba...  \n",
       "28  https://www.cnbc.com/2022/09/02/fashion-retail...  \n",
       "29  https://www.cnbc.com/2022/09/02/high-yield-sav...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url= \"https://www.cnbc.com/world/?region=world\"\n",
    "page= requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "##scrap and parse headlines\n",
    "\n",
    "headline = soup.find_all('a', class_= \"LatestNews-headline\")\n",
    "headlines=[]\n",
    "for i in headline:\n",
    "    i= i.text.strip()\n",
    "    headlines.append(i)\n",
    "    \n",
    "##scrap and parse link\n",
    "\n",
    "link = soup.find_all('a', class_= \"LatestNews-headline\")\n",
    "links=[]\n",
    "for i in link:\n",
    "    links.append(i.get('href'))\n",
    "    \n",
    "##scrap and parse time\n",
    "\n",
    "time= soup.find_all(\"time\")\n",
    "times=[]\n",
    "for i in time:\n",
    "    times.append(i.text)\n",
    "\n",
    "#display\n",
    "\n",
    "data= pd.DataFrame()\n",
    "data[\"Headline\"]= headlines\n",
    "data[\"Time\"]= times\n",
    "data[\"News Link\"]= links\n",
    "data.to_csv(\"News_CNBC.csv\", index= False)\n",
    "print (\"News details from https://www.cnbc.com : \")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ebb90f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09b16e0f",
   "metadata": {},
   "source": [
    "## 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title \n",
    "ii) Authors\n",
    "iii) Published Date \n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5dc5373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "url=\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "page = requests.get(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7dd3e759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the details of most downloaded articles from AI in last 90 days:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper_Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published_Date</th>\n",
       "      <th>Paper_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper_Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published_Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                    Prakken, Henry, Sartor, Giovanni    October 2015   \n",
       "3                                  Boden, Margaret A.     August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                         Miller, Tim   February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11                Bench-Capon, T.J.M., Dunne, Paul E.    October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                       Blum, Avrim L., Langley, Pat   December 1997   \n",
       "15                    Arora, Saurabh, Doshi, Prashant     August 2021   \n",
       "16       Aas, Kjersti, Jullum, Martin, Løland, Anders  September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18     Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.   December 2016   \n",
       "19                       Riveiro, Maria, Thill, Serge  September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                       Kohavi, Ron, John, George H.   December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                    Ying, Mingsheng   February 2010   \n",
       "\n",
       "                                            Paper_URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup= BeautifulSoup(page.content, \"html.parser\")\n",
    "paper_title=[]\n",
    "authors= []\n",
    "published_date=[]\n",
    "paper_url=[]\n",
    "\n",
    "## scrap and parse title\n",
    "title= soup.find_all('h2', class_= \"sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR\")\n",
    "for i in title:\n",
    "    paper_title.append(i.text.strip())\n",
    "\n",
    "## scrap and parse author\n",
    "\n",
    "author = soup.find_all(\"span\", class_= \"sc-1w3fpd7-0 pgLAT\")\n",
    "for i in author:\n",
    "    authors.append(i.text.strip())\n",
    "\n",
    "## scrap and parse published_date\n",
    "\n",
    "date = soup.find_all(\"span\", class_= \"sc-1thf9ly-2 bKddwo\")\n",
    "for i in date:\n",
    "    published_date.append(i.text.strip())\n",
    "\n",
    "## scrap and parse url\n",
    "\n",
    "urls= soup.find_all('li', class_= \"sc-9zxyh7-1 sc-9zxyh7-2 exAXfr jQmQZp\" )\n",
    "for i in urls:\n",
    "    paper_url.append(i.a['href'])\n",
    "    \n",
    "#Display\n",
    "\n",
    "data= pd.DataFrame()\n",
    "data[\"Paper_Title\"] = paper_title\n",
    "data[\"Authors\"] = authors\n",
    "data[\"Published_Date\"] = published_date\n",
    "data[\"Paper_URL\"]= paper_url\n",
    "\n",
    "data.to_csv(\"top 90 books.csv\", index= False)\n",
    "print (\"the details of most downloaded articles from AI in last 90 days:\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c56f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebb52b41",
   "metadata": {},
   "source": [
    "## 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location \n",
    "iv) Ratings\n",
    "v) Image URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b592e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape mentioned details of restaurants of Kolkata from dineout.co.in: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pawan Putra</td>\n",
       "      <td>Chinese, North Indian, Continental</td>\n",
       "      <td>Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zobet</td>\n",
       "      <td>Modern Indian, Sushi, Fast Food, Asian</td>\n",
       "      <td>Fort Knox Mall,Camac Street, Central Kolkata</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jungle Safari</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Mani Square,Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Caafe 360 Degree</td>\n",
       "      <td>Fast Food, Chinese, Italian</td>\n",
       "      <td>Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fly Kouzina</td>\n",
       "      <td>North Indian, Chinese, Continental</td>\n",
       "      <td>Salt Lake, East Kolkata</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zaairah</td>\n",
       "      <td>North Indian, Chinese, Italian</td>\n",
       "      <td>Salt Lake, East Kolkata</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Goofys Cafe</td>\n",
       "      <td>North Indian, Chinese, Italian, Continental, ...</td>\n",
       "      <td>Maniktala, North Kolkata</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Flame &amp; Grill</td>\n",
       "      <td>North Indian, Bengali</td>\n",
       "      <td>Mani Square,Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>K Local</td>\n",
       "      <td>Finger Food, North Indian, Fast Food</td>\n",
       "      <td>City Centre 1,Salt Lake, East Kolkata</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Danish Restaurant</td>\n",
       "      <td>Mughlai, North Indian</td>\n",
       "      <td>Machuabazar, North Kolkata</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fantastic Food Story</td>\n",
       "      <td>Chinese, North Indian, Continental</td>\n",
       "      <td>Hati Bagan, North Kolkata</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Huff N Puff</td>\n",
       "      <td>Chinese, Fast Food, Continental, Pizza, North...</td>\n",
       "      <td>Phool Bagan, North Kolkata</td>\n",
       "      <td>4.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Haka</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Mani Square,Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Uno Pizzeria &amp; Grill</td>\n",
       "      <td>American, Italian, Continental</td>\n",
       "      <td>Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Tunnel</td>\n",
       "      <td>Chinese, North Indian, Continental</td>\n",
       "      <td>Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Bikers Cafe</td>\n",
       "      <td>Continental, Italian, North Indian</td>\n",
       "      <td>Platinum Mall,Elgin Road, South Kolkata</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Sugarr and Spice</td>\n",
       "      <td>Desserts</td>\n",
       "      <td>Maniktala, North Kolkata</td>\n",
       "      <td>3.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Go Green</td>\n",
       "      <td>Italian, North Indian, Chinese</td>\n",
       "      <td>Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Loop Cafe and Restaurant</td>\n",
       "      <td>Italian, Chinese, Fast Food, Mexican</td>\n",
       "      <td>Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Monginis</td>\n",
       "      <td>Desserts, Fast Food</td>\n",
       "      <td>Kankurgachhi, East Kolkata</td>\n",
       "      <td>4.5</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Calcutta 64</td>\n",
       "      <td>Continental, Italian, Beverages, Fast Food</td>\n",
       "      <td>Salt Lake, East Kolkata</td>\n",
       "      <td>4.4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Restaurant name  \\\n",
       "0                    Pawan Putra   \n",
       "1                          Zobet   \n",
       "2                  Jungle Safari   \n",
       "3               Caafe 360 Degree   \n",
       "4                    Fly Kouzina   \n",
       "5                        Zaairah   \n",
       "6                    Goofys Cafe   \n",
       "7                  Flame & Grill   \n",
       "8                        K Local   \n",
       "9              Danish Restaurant   \n",
       "10          Fantastic Food Story   \n",
       "11                   Huff N Puff   \n",
       "12                          Haka   \n",
       "13          Uno Pizzeria & Grill   \n",
       "14                    The Tunnel   \n",
       "15               The Bikers Cafe   \n",
       "16          The Sugarr and Spice   \n",
       "17                      Go Green   \n",
       "18  The Loop Cafe and Restaurant   \n",
       "19                      Monginis   \n",
       "20                   Calcutta 64   \n",
       "\n",
       "                                              Cuisine  \\\n",
       "0                  Chinese, North Indian, Continental   \n",
       "1              Modern Indian, Sushi, Fast Food, Asian   \n",
       "2                               North Indian, Italian   \n",
       "3                         Fast Food, Chinese, Italian   \n",
       "4                  North Indian, Chinese, Continental   \n",
       "5                      North Indian, Chinese, Italian   \n",
       "6    North Indian, Chinese, Italian, Continental, ...   \n",
       "7                               North Indian, Bengali   \n",
       "8                Finger Food, North Indian, Fast Food   \n",
       "9                               Mughlai, North Indian   \n",
       "10                 Chinese, North Indian, Continental   \n",
       "11   Chinese, Fast Food, Continental, Pizza, North...   \n",
       "12                                            Chinese   \n",
       "13                     American, Italian, Continental   \n",
       "14                 Chinese, North Indian, Continental   \n",
       "15                 Continental, Italian, North Indian   \n",
       "16                                           Desserts   \n",
       "17                     Italian, North Indian, Chinese   \n",
       "18               Italian, Chinese, Fast Food, Mexican   \n",
       "19                                Desserts, Fast Food   \n",
       "20         Continental, Italian, Beverages, Fast Food   \n",
       "\n",
       "                                        Location Ratings  \\\n",
       "0                     Kankurgachhi, East Kolkata     4.2   \n",
       "1   Fort Knox Mall,Camac Street, Central Kolkata     4.4   \n",
       "2         Mani Square,Kankurgachhi, East Kolkata     4.3   \n",
       "3                     Kankurgachhi, East Kolkata     4.5   \n",
       "4                        Salt Lake, East Kolkata     4.4   \n",
       "5                        Salt Lake, East Kolkata     4.2   \n",
       "6                       Maniktala, North Kolkata     4.4   \n",
       "7         Mani Square,Kankurgachhi, East Kolkata     4.3   \n",
       "8          City Centre 1,Salt Lake, East Kolkata       4   \n",
       "9                     Machuabazar, North Kolkata     4.4   \n",
       "10                     Hati Bagan, North Kolkata     4.3   \n",
       "11                    Phool Bagan, North Kolkata     4.8   \n",
       "12        Mani Square,Kankurgachhi, East Kolkata     4.2   \n",
       "13                    Kankurgachhi, East Kolkata     4.4   \n",
       "14                    Kankurgachhi, East Kolkata     4.2   \n",
       "15       Platinum Mall,Elgin Road, South Kolkata     4.3   \n",
       "16                      Maniktala, North Kolkata     3.5   \n",
       "17                    Kankurgachhi, East Kolkata     4.2   \n",
       "18                    Kankurgachhi, East Kolkata     4.3   \n",
       "19                    Kankurgachhi, East Kolkata     4.5   \n",
       "20                       Salt Lake, East Kolkata     4.4   \n",
       "\n",
       "                                            Image URL  \n",
       "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
       "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "20  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "url=\"https://www.dineout.co.in/kolkata-restaurants\"\n",
    "page = requests.get(url)\n",
    "soup= BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "## scrap and parse name\n",
    "\n",
    "names=[]\n",
    "name= soup.find_all(\"div\",class_= \"restnt-info cursor\")\n",
    "for i in name:\n",
    "    names.append(i.a.text)\n",
    "\n",
    "## scrap and parse location\n",
    "\n",
    "location= []\n",
    "loc= soup.find_all(\"div\",class_=\"restnt-loc ellipsis\")\n",
    "for i in loc:\n",
    "    location.append(i.text)\n",
    "\n",
    "## scrap and parse cuisine\n",
    "    \n",
    "cuisine= []\n",
    "cou= soup.find_all(\"div\",{'class':\"detail-info\"})\n",
    "for i in cou:\n",
    "    i= i.span.get_text().split(\"|\")[1]\n",
    "    cuisine.append(i)\n",
    "\n",
    "## scrap and parse ratings   \n",
    "\n",
    "ratings= []\n",
    "rat= soup.find_all(\"div\",class_={\"restnt-rating rating-4\",\"restnt-rating rating-5\"})\n",
    "for i in rat:\n",
    "    ratings.append(i.text)    \n",
    "\n",
    "## scrap and parse url\n",
    "    \n",
    "url=[]\n",
    "u= soup.find_all(\"div\",attrs= {\"class\":\"img cursor\" })\n",
    "for i in u:\n",
    "    url.append(i.img[\"data-src\"])\n",
    "    \n",
    "#display\n",
    "\n",
    "data= pd.DataFrame()\n",
    "data[\"Restaurant name\"] = names\n",
    "data[\"Cuisine\"] = cuisine\n",
    "data[\"Location\"] = location\n",
    "data[\"Ratings\"] = ratings\n",
    "data[\" Image URL\"]= url\n",
    "\n",
    "data.to_csv(\"Restaurants_details.csv\", index= False)\n",
    "\n",
    "print (\"Scrape mentioned details of restaurants of Kolkata from dineout.co.in: \")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace4e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "064634f7",
   "metadata": {},
   "source": [
    "## 10) Write a python program to scrape the details of top publications from Google Scholar from https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank \n",
    "ii) Publication\n",
    "iii) h5-index\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1274c8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The details of top publications from Google Scholar : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5-index</th>\n",
       "      <th>h5-median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5-index h5-median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "url=\"https://scholar.google.com/citations?view_op=top_venues&hl=en\"\n",
    "page = requests.get(url)\n",
    "soup= BeautifulSoup(page.content, \"html.parser\")\n",
    "ranks=[]\n",
    "publication= []\n",
    "h5index= []\n",
    "h5median= []\n",
    "\n",
    "## scrap and parse rank\n",
    "rank= soup.find_all(\"td\",class_= \"gsc_mvt_p\")\n",
    "for i in rank:\n",
    "    ranks.append(i.text)\n",
    "    \n",
    "## scrap and parse publication\n",
    "\n",
    "pub= soup.find_all(\"td\",class_= \"gsc_mvt_t\")\n",
    "for i in pub:\n",
    "    publication.append(i.text.strip())\n",
    "    \n",
    "h5= soup.find_all(\"td\",class_= \"gsc_mvt_n\")\n",
    "h5im=[]\n",
    "for i in h5:\n",
    "    h5im.append(i.text.strip())\n",
    "\n",
    "## scrap and parse h5 index\n",
    "\n",
    "for j in range(0, len(h5im)): \n",
    "    if j % 2== 0:\n",
    "        h5index.append(h5im[j])\n",
    "\n",
    "## scrap and parse h5 median\n",
    "\n",
    "for j in range(0, len(h5im)): \n",
    "    if j % 2== 1:\n",
    "        h5median.append(h5im[j])\n",
    "        \n",
    "#Display\n",
    "\n",
    "data= pd.DataFrame()\n",
    "data[\"Rank\"] = ranks\n",
    "data[\"Publication\"] = publication\n",
    "data[\"h5-index\"] = h5index\n",
    "data[\"h5-median\"]= h5median\n",
    "\n",
    "print(\"The details of top publications from Google Scholar : \\n\")\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f5b78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
